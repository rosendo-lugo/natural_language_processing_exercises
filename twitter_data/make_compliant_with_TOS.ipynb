{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset TOS Compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data/congress_20161107-210118.csv\n",
      "full_data/congress_20161107-210134.csv\n",
      "full_data/congress_20161108-053855.csv\n",
      "full_data/congress_20161108-053906.csv\n",
      "full_data/congress_20161108-132431.csv\n",
      "full_data/congress_20161108-132444.csv\n",
      "full_data/congress_20161108-162723.csv\n",
      "full_data/congress_20161108-162733.csv\n",
      "full_data/democrat_20161107-210107.csv\n",
      "full_data/democrat_20161108-053848.csv\n",
      "full_data/democrat_20161108-132423.csv\n",
      "full_data/democrat_20161108-162714.csv\n",
      "full_data/election2016_20161107-210044.csv\n",
      "full_data/election2016_20161108-053835.csv\n",
      "full_data/election2016_20161108-132408.csv\n",
      "full_data/election2016_20161108-162659.csv\n",
      "full_data/election_20161107-210035.csv\n",
      "full_data/election_20161108-053622.csv\n",
      "full_data/election_20161108-132405.csv\n",
      "full_data/election_20161108-162655.csv\n",
      "full_data/electionday_20161107-210050.csv\n",
      "full_data/electionday_20161108-053838.csv\n",
      "full_data/electionday_20161108-132412.csv\n",
      "full_data/electionday_20161108-162703.csv\n",
      "full_data/gop_20161107-210101.csv\n",
      "full_data/gop_20161108-053845.csv\n",
      "full_data/gop_20161108-132420.csv\n",
      "full_data/gop_20161108-162710.csv\n",
      "full_data/hillary_20161107-210019.csv\n",
      "full_data/hillary_20161108-053608.csv\n",
      "full_data/hillary_20161108-132354.csv\n",
      "full_data/hillary_20161108-162643.csv\n",
      "full_data/house_20161107-210129.csv\n",
      "full_data/house_20161108-053903.csv\n",
      "full_data/house_20161108-132441.csv\n",
      "full_data/house_20161108-162729.csv\n",
      "full_data/madampresident_20161107-210139.csv\n",
      "full_data/madampresident_20161108-053911.csv\n",
      "full_data/madampresident_20161108-132448.csv\n",
      "full_data/madampresident_20161108-162737.csv\n",
      "full_data/trump_20161107-210024.csv\n",
      "full_data/trump_20161108-053612.csv\n",
      "full_data/trump_20161108-132357.csv\n",
      "full_data/trump_20161108-162647.csv\n",
      "full_data/uselection_20161107-210124.csv\n",
      "full_data/uselection_20161108-053900.csv\n",
      "full_data/uselection_20161108-132436.csv\n",
      "full_data/uselection_20161108-162726.csv\n",
      "full_data/uselections2016_20161107-210055.csv\n",
      "full_data/uselections2016_20161108-053841.csv\n",
      "full_data/uselections2016_20161108-132415.csv\n",
      "full_data/uselections2016_20161108-162706.csv\n",
      "full_data/voted_20161107-210113.csv\n",
      "full_data/voted_20161108-053852.csv\n",
      "full_data/voted_20161108-132427.csv\n",
      "full_data/voted_20161108-162718.csv\n",
      "full_data/yourefired_20161107-210029.csv\n",
      "full_data/yourefired_20161108-053617.csv\n",
      "full_data/yourefired_20161108-132400.csv\n",
      "full_data/yourefired_20161108-162651.csv\n",
      "Creating dataframe...\n",
      "Removing duplicates...\n",
      "6546824\n",
      "Saving dataframe...\n"
     ]
    }
   ],
   "source": [
    "def clean_datasets():\n",
    "    \"\"\" \n",
    "    Takes the full datasets scraped from each miner and outputs a clean dataset\n",
    "    containing only the tweet IDs. This makes it compliant with Twitter's TOS.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = 'full_data/'\n",
    "    \n",
    "    tweet_ids = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(path, '*.csv')):\n",
    "        print(filename)\n",
    "        try:\n",
    "            df_file = pd.read_csv(filename, usecols=['id'],  dtype= {'id': int})\n",
    "            tweet_ids.extend(df_file['id'].values.tolist())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "     \n",
    "    print('Creating dataframe...')\n",
    "    df = pd.DataFrame({'tweet_id':tweet_ids})\n",
    "    print('Removing duplicates...')\n",
    "    df = df.drop_duplicates()\n",
    "    print(len(df))\n",
    "    print('Saving dataframe...')\n",
    "    df.to_csv(\"clean_data/clean_data.csv\", index=False)\n",
    "    \n",
    "clean_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split(filehandler, delimiter=',', row_limit=1000000, \n",
    "    output_name_template='clean_data/tweets_%s.csv', output_path='.', keep_headers=True):\n",
    "    \"\"\"\n",
    "    A quick bastardization of the Python CSV library.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `row_limit`: The number of rows you want in each output file. 10,000 by default.\n",
    "        `output_name_template`: A %s-style template for the numbered output files.\n",
    "        `output_path`: Where to stick the output files.\n",
    "        `keep_headers`: Whether or not to print the headers in each output file.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "        >> from toolbox import csv_splitter;\n",
    "        >> csv_splitter.split(open('/home/ben/input.csv', 'r'));\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    reader = csv.reader(filehandler, delimiter=delimiter)\n",
    "    current_piece = 1\n",
    "    current_out_path = os.path.join(\n",
    "         output_path,\n",
    "         output_name_template  % current_piece\n",
    "    )\n",
    "    current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "    current_limit = row_limit\n",
    "    if keep_headers:\n",
    "        headers = next(reader)\n",
    "        current_out_writer.writerow(headers)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i + 1 > current_limit:\n",
    "            current_piece += 1\n",
    "            current_limit = row_limit * current_piece\n",
    "            current_out_path = os.path.join(\n",
    "               output_path,\n",
    "               output_name_template  % current_piece\n",
    "            )\n",
    "            current_out_writer = csv.writer(open(current_out_path, 'w'), delimiter=delimiter)\n",
    "            if keep_headers:\n",
    "                current_out_writer.writerow(headers)\n",
    "        current_out_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split(open('clean_data/clean_data.csv', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
